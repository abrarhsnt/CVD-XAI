{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjI0JZCt-Do6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9c8133d-b5f0-4e66-a39f-b9dc3fc1fd6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading processed data...\n",
            "Training set shape: (183824, 25)\n",
            "Test set shape: (45957, 25)\n",
            "Positive class ratio: 0.103\n",
            "Class weights: {0: np.float64(0.5575492872308159), 1: np.float64(4.84410245599241)}\n",
            "\n",
            "================================================================================\n",
            "1. LOGISTIC REGRESSION BASELINE\n",
            "================================================================================\n",
            "\n",
            "============================================================\n",
            "Evaluating Logistic Regression\n",
            "============================================================\n",
            "Training AUC: 0.8358\n",
            "Test AUC: 0.8355\n",
            "Test F1-Score: 0.3813\n",
            "Average Precision: 0.3725\n",
            "CV AUC: 0.8356 (±0.0026)\n",
            "CV F1: 0.3835 (±0.0024)\n",
            "Time taken: 4.55 seconds\n",
            "\n",
            "================================================================================\n",
            "2. RANDOM FOREST BASELINE\n",
            "================================================================================\n",
            "\n",
            "============================================================\n",
            "Evaluating Random Forest\n",
            "============================================================\n",
            "Training AUC: 0.8570\n",
            "Test AUC: 0.8327\n",
            "Test F1-Score: 0.3780\n",
            "Average Precision: 0.3598\n",
            "CV AUC: 0.8321 (±0.0033)\n",
            "CV F1: 0.3827 (±0.0037)\n",
            "Time taken: 12.39 seconds\n",
            "\n",
            "================================================================================\n",
            "3. XGBOOST BASELINE\n",
            "================================================================================\n",
            "\n",
            "============================================================\n",
            "Evaluating XGBoost\n",
            "============================================================\n",
            "Training AUC: 0.8563\n",
            "Test AUC: 0.8372\n",
            "Test F1-Score: 0.3774\n",
            "Average Precision: 0.3732\n",
            "CV AUC: 0.8362 (±0.0024)\n",
            "CV F1: 0.3802 (±0.0011)\n",
            "Time taken: 285.58 seconds\n",
            "\n",
            "================================================================================\n",
            "4. LIGHTGBM BASELINE\n",
            "================================================================================\n",
            "\n",
            "============================================================\n",
            "Evaluating LightGBM\n",
            "============================================================\n",
            "[LightGBM] [Info] Number of positive: 18974, number of negative: 164850\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005062 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 305\n",
            "[LightGBM] [Info] Number of data points in the train set: 183824, number of used features: 24\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
            "[LightGBM] [Info] Start training from score -0.000000\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Training AUC: 0.8497\n",
            "Test AUC: 0.8385\n",
            "Test F1-Score: 0.3777\n",
            "Average Precision: 0.3758\n",
            "CV AUC: 0.8375 (±0.0024)\n",
            "CV F1: 0.3801 (±0.0011)\n",
            "Time taken: 10.05 seconds\n",
            "\n",
            "================================================================================\n",
            "MODEL COMPARISON SUMMARY\n",
            "================================================================================\n",
            "\n",
            "Detailed Results:\n",
            "              Model  Train_AUC  Test_AUC  Test_F1  Test_AP  CV_AUC_Mean  CV_AUC_Std  CV_F1_Mean  CV_F1_Std  Time_Seconds\n",
            "Logistic Regression     0.8358    0.8355   0.3813   0.3725       0.8356      0.0026      0.3835     0.0024          4.55\n",
            "      Random Forest     0.8570    0.8327   0.3780   0.3598       0.8321      0.0033      0.3827     0.0037         12.39\n",
            "            XGBoost     0.8563    0.8372   0.3774   0.3732       0.8362      0.0024      0.3802     0.0011        285.58\n",
            "           LightGBM     0.8497    0.8385   0.3777   0.3758       0.8375      0.0024      0.3801     0.0011         10.05\n",
            "\n",
            " Best Model: LightGBM (Test AUC: 0.8385)\n",
            "\n",
            "================================================================================\n",
            "SAVING MODELS AND RESULTS\n",
            "================================================================================\n",
            "Saved Logistic Regression to ../models/saved_models/baseline_logistic_regression.pkl\n",
            "Saved Random Forest to ../models/saved_models/baseline_random_forest.pkl\n",
            "Saved XGBoost to ../models/saved_models/baseline_xgboost.pkl\n",
            "Saved LightGBM to ../models/saved_models/baseline_lightgbm.pkl\n",
            "Best model (LightGBM) saved separately\n",
            "Results saved to CSV\n",
            "✓ Detailed report generated\n",
            "\n",
            "================================================================================\n",
            "BASELINE MODELING COMPLETED SUCCESSFULLY!\n",
            "================================================================================\n",
            "Best performing model: LightGBM\n",
            "Best Test AUC: 0.8385\n",
            "Visualization files saved in '../reports/figures/'\n",
            "Models saved in '../models/saved_models/'\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "from sklearn.metrics import (roc_auc_score, f1_score, classification_report,\n",
        "                             confusion_matrix, roc_curve, precision_recall_curve,\n",
        "                             average_precision_score)\n",
        "import joblib\n",
        "import os\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "\n",
        "os.makedirs(\"../models/saved_models\", exist_ok=True)\n",
        "os.makedirs(\"../reports/figures\", exist_ok=True)\n",
        "\n",
        "def evaluate_model(model, X_train, y_train, X_test, y_test, model_name, cv_folds=5):\n",
        "    \"\"\"\n",
        "    Comprehensive model evaluation with cross-validation and multiple metrics\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Evaluating {model_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Time tracking\n",
        "    start_time = datetime.now()\n",
        "\n",
        "    # Cross-validation\n",
        "    cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
        "    cv_scores_auc = cross_val_score(model, X_train, y_train,\n",
        "                                   cv=cv, scoring='roc_auc', n_jobs=-1)\n",
        "    cv_scores_f1 = cross_val_score(model, X_train, y_train,\n",
        "                                  cv=cv, scoring='f1', n_jobs=-1)\n",
        "\n",
        "    # Train model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predictions\n",
        "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate metrics\n",
        "    train_auc = roc_auc_score(y_train, model.predict_proba(X_train)[:, 1])\n",
        "    test_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "    test_f1 = f1_score(y_test, y_pred)\n",
        "    test_ap = average_precision_score(y_test, y_pred_proba)\n",
        "\n",
        "    # Time taken\n",
        "    end_time = datetime.now()\n",
        "    time_taken = (end_time - start_time).total_seconds()\n",
        "\n",
        "    # Create results dictionary\n",
        "    results = {\n",
        "        'Model': model_name,\n",
        "        'Train_AUC': round(train_auc, 4),\n",
        "        'Test_AUC': round(test_auc, 4),\n",
        "        'Test_F1': round(test_f1, 4),\n",
        "        'Test_AP': round(test_ap, 4),\n",
        "        'CV_AUC_Mean': round(cv_scores_auc.mean(), 4),\n",
        "        'CV_AUC_Std': round(cv_scores_auc.std(), 4),\n",
        "        'CV_F1_Mean': round(cv_scores_f1.mean(), 4),\n",
        "        'CV_F1_Std': round(cv_scores_f1.std(), 4),\n",
        "        'Time_Seconds': round(time_taken, 2)\n",
        "    }\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Training AUC: {train_auc:.4f}\")\n",
        "    print(f\"Test AUC: {test_auc:.4f}\")\n",
        "    print(f\"Test F1-Score: {test_f1:.4f}\")\n",
        "    print(f\"Average Precision: {test_ap:.4f}\")\n",
        "    print(f\"CV AUC: {cv_scores_auc.mean():.4f} (±{cv_scores_auc.std():.4f})\")\n",
        "    print(f\"CV F1: {cv_scores_f1.mean():.4f} (±{cv_scores_f1.std():.4f})\")\n",
        "    print(f\"Time taken: {time_taken:.2f} seconds\")\n",
        "\n",
        "    # Generate plots\n",
        "    generate_model_plots(model, X_test, y_test, y_pred, y_pred_proba, model_name)\n",
        "\n",
        "    return results, model\n",
        "\n",
        "def generate_model_plots(model, X_test, y_test, y_pred, y_pred_proba, model_name):\n",
        "    \"\"\"\n",
        "    Generate comprehensive evaluation plots for each model\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    fig.suptitle(f'{model_name} - Model Evaluation', fontsize=16, fontweight='bold')\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0, 0])\n",
        "    axes[0, 0].set_title('Confusion Matrix')\n",
        "    axes[0, 0].set_xlabel('Predicted')\n",
        "    axes[0, 0].set_ylabel('Actual')\n",
        "\n",
        "    # ROC Curve\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
        "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "    axes[0, 1].plot(fpr, tpr, color='darkorange', lw=2,\n",
        "                   label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
        "    axes[0, 1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    axes[0, 1].set_xlim([0.0, 1.0])\n",
        "    axes[0, 1].set_ylim([0.0, 1.05])\n",
        "    axes[0, 1].set_xlabel('False Positive Rate')\n",
        "    axes[0, 1].set_ylabel('True Positive Rate')\n",
        "    axes[0, 1].set_title('ROC Curve')\n",
        "    axes[0, 1].legend(loc=\"lower right\")\n",
        "\n",
        "    # Precision-Recall Curve\n",
        "    precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
        "    ap = average_precision_score(y_test, y_pred_proba)\n",
        "    axes[1, 0].plot(recall, precision, color='green', lw=2,\n",
        "                   label=f'PR curve (AP = {ap:.4f})')\n",
        "    axes[1, 0].set_xlim([0.0, 1.0])\n",
        "    axes[1, 0].set_ylim([0.0, 1.05])\n",
        "    axes[1, 0].set_xlabel('Recall')\n",
        "    axes[1, 0].set_ylabel('Precision')\n",
        "    axes[1, 0].set_title('Precision-Recall Curve')\n",
        "    axes[1, 0].legend(loc=\"upper right\")\n",
        "\n",
        "    # Feature Importance (if available)\n",
        "    if hasattr(model, 'feature_importances_'):\n",
        "        feature_importances = pd.DataFrame({\n",
        "            'feature': range(len(model.feature_importances_)),\n",
        "            'importance': model.feature_importances_\n",
        "        })\n",
        "        feature_importances = feature_importances.sort_values('importance', ascending=False).head(10)\n",
        "        axes[1, 1].barh(feature_importances['feature'], feature_importances['importance'])\n",
        "        axes[1, 1].set_xlabel('Importance')\n",
        "        axes[1, 1].set_ylabel('Feature Index')\n",
        "        axes[1, 1].set_title('Top 10 Feature Importances')\n",
        "    else:\n",
        "        # For logistic regression, show coefficients\n",
        "        if hasattr(model, 'coef_'):\n",
        "            coef = pd.DataFrame({\n",
        "                'feature': range(len(model.coef_[0])),\n",
        "                'coefficient': model.coef_[0]\n",
        "            })\n",
        "            coef['abs_coef'] = np.abs(coef['coefficient'])\n",
        "            coef = coef.sort_values('abs_coef', ascending=False).head(10)\n",
        "            axes[1, 1].barh(coef['feature'], coef['coefficient'])\n",
        "            axes[1, 1].set_xlabel('Coefficient Value')\n",
        "            axes[1, 1].set_ylabel('Feature Index')\n",
        "            axes[1, 1].set_title('Top 10 Feature Coefficients (Logistic Regression)')\n",
        "        else:\n",
        "            axes[1, 1].text(0.5, 0.5, 'Feature importance\\nnot available',\n",
        "                           ha='center', va='center', transform=axes[1, 1].transAxes)\n",
        "            axes[1, 1].set_title('Feature Importance')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"../reports/figures/{model_name.replace(' ', '_').lower()}_evaluation.svg\",\n",
        "                format='svg', bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "def plot_comparison_results(results_df):\n",
        "    \"\"\"\n",
        "    Create comprehensive comparison plots for all models\n",
        "    \"\"\"\n",
        "    # Metrics comparison\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    fig.suptitle('Baseline Models Comparison', fontsize=16, fontweight='bold')\n",
        "\n",
        "    metrics = ['Test_AUC', 'Test_F1', 'CV_AUC_Mean', 'Time_Seconds']\n",
        "    titles = ['Test AUC Score', 'Test F1-Score', 'Cross-Validation AUC', 'Training Time (seconds)']\n",
        "\n",
        "    for idx, (metric, title) in enumerate(zip(metrics, titles)):\n",
        "        ax = axes[idx//2, idx%2]\n",
        "        bars = ax.bar(results_df['Model'], results_df[metric], color=plt.cm.Set3(np.arange(len(results_df))))\n",
        "        ax.set_title(title, fontweight='bold')\n",
        "        ax.set_ylabel(metric.replace('_', ' '))\n",
        "        ax.tick_params(axis='x', rotation=45)\n",
        "\n",
        "        # Add value labels on bars\n",
        "        for bar in bars:\n",
        "            height = bar.get_height()\n",
        "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                   f'{height:.4f}' if metric != 'Time_Seconds' else f'{height:.1f}s',\n",
        "                   ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"../reports/figures/baseline_models_comparison.svg\",\n",
        "                format='svg', bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # Detailed metrics plot\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    metrics_plot = ['Test_AUC', 'Test_F1', 'Test_AP', 'CV_AUC_Mean']\n",
        "    x_pos = np.arange(len(results_df))\n",
        "\n",
        "    for i, metric in enumerate(metrics_plot):\n",
        "        plt.bar(x_pos + i*0.2, results_df[metric], width=0.2,\n",
        "               label=metric.replace('_', ' '))\n",
        "\n",
        "    plt.xlabel('Models')\n",
        "    plt.ylabel('Scores')\n",
        "    plt.title('Detailed Model Performance Comparison', fontweight='bold')\n",
        "    plt.xticks(x_pos + 0.3, results_df['Model'], rotation=45)\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"../reports/figures/detailed_performance_comparison.svg\",\n",
        "                format='svg', bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "# Load processed data\n",
        "print(\"Loading processed data...\")\n",
        "X_train = np.load(\"../data/processed/X_train_scaled.npy\")\n",
        "X_test = np.load(\"../data/processed/X_test_scaled.npy\")\n",
        "y_train = np.load(\"../data/processed/y_train.npy\")\n",
        "y_test = np.load(\"../data/processed/y_test.npy\")\n",
        "\n",
        "print(f\"Training set shape: {X_train.shape}\")\n",
        "print(f\"Test set shape: {X_test.shape}\")\n",
        "print(f\"Positive class ratio: {y_train.mean():.3f}\")\n",
        "\n",
        "# Calculate class weights\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weight_dict = dict(enumerate(class_weights))\n",
        "print(f\"Class weights: {class_weight_dict}\")\n",
        "\n",
        "# Initialize results storage\n",
        "all_results = []\n",
        "trained_models = {}\n",
        "\n",
        "# Logistic Regression\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"1. LOGISTIC REGRESSION BASELINE\")\n",
        "print(\"=\"*80)\n",
        "lr_model = LogisticRegression(\n",
        "    class_weight='balanced',\n",
        "    random_state=42,\n",
        "    max_iter=1000,\n",
        "    solver='liblinear',\n",
        "    C=1.0\n",
        ")\n",
        "lr_results, lr_trained = evaluate_model(lr_model, X_train, y_train, X_test, y_test, \"Logistic Regression\")\n",
        "all_results.append(lr_results)\n",
        "trained_models['Logistic Regression'] = lr_trained\n",
        "\n",
        "# Random Forest\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"2. RANDOM FOREST BASELINE\")\n",
        "print(\"=\"*80)\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    class_weight='balanced',\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    max_depth=10,\n",
        "    min_samples_split=5\n",
        ")\n",
        "rf_results, rf_trained = evaluate_model(rf_model, X_train, y_train, X_test, y_test, \"Random Forest\")\n",
        "all_results.append(rf_results)\n",
        "trained_models['Random Forest'] = rf_trained\n",
        "\n",
        "# XGBoost\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"3. XGBOOST BASELINE\")\n",
        "print(\"=\"*80)\n",
        "scale_pos_weight = sum(y_train == 0) / sum(y_train == 1)\n",
        "xgb_model = XGBClassifier(\n",
        "    scale_pos_weight=scale_pos_weight,\n",
        "    random_state=42,\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='logloss',\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=6\n",
        ")\n",
        "xgb_results, xgb_trained = evaluate_model(xgb_model, X_train, y_train, X_test, y_test, \"XGBoost\")\n",
        "all_results.append(xgb_results)\n",
        "trained_models['XGBoost'] = xgb_trained\n",
        "\n",
        "# LightGBM\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"4. LIGHTGBM BASELINE\")\n",
        "print(\"=\"*80)\n",
        "lgbm_model = LGBMClassifier(\n",
        "    class_weight='balanced',\n",
        "    random_state=42,\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=6,\n",
        "    n_jobs=-1\n",
        ")\n",
        "lgbm_results, lgbm_trained = evaluate_model(lgbm_model, X_train, y_train, X_test, y_test, \"LightGBM\")\n",
        "all_results.append(lgbm_results)\n",
        "trained_models['LightGBM'] = lgbm_trained\n",
        "\n",
        "# Compare results\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MODEL COMPARISON SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "results_df = pd.DataFrame(all_results)\n",
        "print(\"\\nDetailed Results:\")\n",
        "print(results_df.to_string(index=False))\n",
        "\n",
        "# Sort by Test AUC to find best model\n",
        "results_df_sorted = results_df.sort_values('Test_AUC', ascending=False)\n",
        "best_model_name = results_df_sorted.iloc[0]['Model']\n",
        "best_model = trained_models[best_model_name]\n",
        "\n",
        "print(f\"\\n Best Model: {best_model_name} (Test AUC: {results_df_sorted.iloc[0]['Test_AUC']:.4f})\")\n",
        "\n",
        "# Generate comparison plots\n",
        "plot_comparison_results(results_df)\n",
        "\n",
        "# Save all models and results\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SAVING MODELS AND RESULTS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Save all models\n",
        "for model_name, model in trained_models.items():\n",
        "    filename = f\"../models/saved_models/baseline_{model_name.replace(' ', '_').lower()}.pkl\"\n",
        "    joblib.dump(model, filename)\n",
        "    print(f\"Saved {model_name} to {filename}\")\n",
        "\n",
        "# Save best model separately\n",
        "joblib.dump(best_model, \"../models/saved_models/best_baseline_model.pkl\")\n",
        "print(f\"Best model ({best_model_name}) saved separately\")\n",
        "\n",
        "# Save results to CSV\n",
        "results_df.to_csv(\"../models/saved_models/baseline_model_results.csv\", index=False)\n",
        "print(\"Results saved to CSV\")\n",
        "\n",
        "# Save detailed comparison report\n",
        "with open(\"../models/saved_models/baseline_models_report.txt\", \"w\") as f:\n",
        "    f.write(\"BASELINE MODELS COMPARISON REPORT\\n\")\n",
        "    f.write(\"=\" * 50 + \"\\n\")\n",
        "    f.write(f\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "    f.write(f\"Dataset: {X_train.shape[0]:,} training samples, {X_test.shape[0]:,} test samples\\n\")\n",
        "    f.write(f\"Positive class ratio: {y_train.mean():.3f}\\n\\n\")\n",
        "\n",
        "    f.write(\"PERFORMANCE SUMMARY:\\n\")\n",
        "    f.write(\"-\" * 30 + \"\\n\")\n",
        "    for _, row in results_df_sorted.iterrows():\n",
        "        f.write(f\"{row['Model']}:\\n\")\n",
        "        f.write(f\"  Test AUC: {row['Test_AUC']:.4f}\\n\")\n",
        "        f.write(f\"  Test F1: {row['Test_F1']:.4f}\\n\")\n",
        "        f.write(f\"  CV AUC: {row['CV_AUC_Mean']:.4f} (±{row['CV_AUC_Std']:.4f})\\n\")\n",
        "        f.write(f\"  Time: {row['Time_Seconds']:.1f}s\\n\\n\")\n",
        "\n",
        "print(\"Detailed report generated\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"BASELINE MODELING COMPLETED SUCCESSFULLY!\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Best performing model: {best_model_name}\")\n",
        "print(f\"Best Test AUC: {results_df_sorted.iloc[0]['Test_AUC']:.4f}\")\n",
        "print(f\"Visualization files saved in '../reports/figures/'\")\n",
        "print(f\"Models saved in '../models/saved_models/'\")"
      ]
    }
  ]
}